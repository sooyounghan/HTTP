-----
### 루프와 중복 피하기
-----
1. 모든 순환을 피하는 완벽한 방법은 없음 : 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱 집합을 필요로 함
2. 휴리스틱은 문제를 피하는데 도움을 주지만, 동시에 약간의 손실을 유발할 수 있음 : 의심스러워 보이지만 실은 유효한 콘텐츠를 필터링할 수 있게 될 수 있기 때문임
3. 웹은 로봇이 문제를 일으킬 가능성으로 가득차있는데, 웹에서 로봇이 올바르게 동작하기 위해 사용하는 기법
   - URL 정규화 : URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복 URL이 생기는 것을 일부 회피
   - 너비 우선 크롤링 : 크롤러들은 언제든지 크롤링 할 수 있는 URL들의 큰 집합을 가지고 있음
     + 방문할 URL들을 웹 사이트 전체에 걸쳐 너비 우선으로 스케줄링하면, 순환의 영향을 최소화할 수 있음
     + 혹여, 로봇 함정을 건드리게 되더라도, 여전히 그 순환에서 페이지를 받아오기 전 다른 웹 사이트들에서 수십 만 개의 페이지들을 받아올 수 있음
     + 만약, 로봇을 깊이 우선 방식으로 운용하여 웹 사이트 하나에 성급하게 진입하면, 순환을 건드리는 경우 영원히 다른 사이트로 빠져나올 수 없게 될 것

   - 스트롤링 : 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자 제한
     + 만약, 로봇이 순환을 건드려서 지속적으로 그 사이트의 별칭들에 접근을 시도한다면, 스트롤링을 이용해 그 서버에 대한 접근 횟수와 중복 총 횟수 제한할 수 있을 것

   - URL 크기 제한 : 로봇은 일정 길이(보통 1KB)를 넘는 URL 크롤링을 거부할 수 있음
     + 만약, 순환으로 인해 URL이 계속해서 길어진다면, 결국에는 길이 제한으로 인해 순환이 중단될 것
     + 어떤 웹 서버들은 긴 URL이 주어지는 경우 실패하며, URL이 점점 길어지는 순환에 빠진 로봇은 이러한 웹 서버들과 충돌 유발 가능 : 이는 웹 마스터가 로봇을 서비스 공격 거부자로 오해하도록 만듬
     + 주의사항은 이 기법을 적용하면 가져오지 못하는 콘텐츠도 틀림없이 있을 것이라는 점 : 많은 사이트들이 오늘날 URL을 사용자 상태를 관리하기 위해 사용 (예) 사용자 아이디를 페이지에서 참조하고 있는 URL들에 저장)
     + URL 길이는 크롤링을 제한 하는 방법으로서는 까다로운 것이며, 그러나 이 기법은 요청 URL이 특정 크기에 도달할 때마다 에러 로그를 남김으로써, 특정 사이트에서 어떤 일이 벌어지고 있는지 검사하는 사용자에게는 훌륭한 신호 제공

   - URL / 사이트 블랙리스트
     + 로봇 순환을 만들어 내거나 함정인 것처럼 알려진 사이트와 URL 목록을 만들어 관리하고 피하는 것
     + 문제를 일으키는 사이트나 URL이 발견될 때마다 이 블랙리스트에 추가
     + 이는 사람의 손을 필요로 하지만, 오늘날 실제 사용 중 대부분 대규모 크롤러들은 몇 가지 형태의 블랙리스트를 가지고 있으며, 블랙리스트는 크롤링 된느 것을 싫어하는 특정 사이트를 피하기 위해 사용될 수 있음
    
   - 패턴 발견 : 파일 시스템의 심볼릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향 존재
     + 예) URL의 중복된 구성 요소와 함께 길어질 수 있으며, 몇 로봇은 반복되는 구성요소를 가진 URL을 잠재적 순환으로 보고, 둘 중 혹은 셋 이상의 반복된 구성 요소를 갖는 URL을 크롤링하는 것 거절
     + 주기의 길이가 2 이상인 반복도 존재할 수 있으며, 몇 가지 다른 주기의 반복 패턴을 감지하는 로봇도 존재

   - 콘텐츠 지문(Fingerprint)
     + 지문은 더욱 복잡한 웹 크롤러들 몇몇에 의해 사용되는 중복 감지보다 직접적인 방법
     + 콘텐츠 지문을 사용하는 로봇들은 페이지 콘텐츠에서 몇 바이트를 얻어내 체크섬(Checksum) 계산
     + 이 체크섬은 페이지 내용의 간략한 표현이며, 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페이지 링크는 크롤링하지 않음 : 로봇이 그 페이지 콘텐츠를 이전에 본 것이므로, 그 페이지 링크들은 이미 크롤링이 시작된 상태이기 때문임
     + 체크섬 함수는 어떤 두 페이지가 서로 다른 내용임에도 체크섬이 똑같은 확률이 적은 것을 사용해야 함 : 지문 생성용으로는 MD5와 같은 메세지 요약 함수가 인기 있음
     + 어떤 웹 서버들은 동적으로 그때 그때 페이지를 수정하므로, 로봇들은 때떄로 웹 페이지 콘텐츠에 임베딩된 링크와 같은 특정 부분들을 체크섬에서 빠뜨림
     + 뿐만 아니라 페이지 콘텐츠를 임의로 커스터마이징하는 것(날짜 추가, 카운터 접근 등)을 포함한 서버 측의 동적 동작을 중복 감지 방해 가능

   - 사람의 모니터링
     + 로봇은 결국 자신에게 어떤 기법으로도 해결할 수 없는 문제에 봉착할 텐데, 모든 상용수준의 로봇은 사람이 쉽게 로봇의 진행 상황을 모니터링해서 특별한 일이 일어나면 즉각 인지하게끔 반드시 진단과 로깅을 포함해서 설계되어야 함

4. 웹과 거대한 데이터 집합을 크롤링하기 위해 좋은 스파이더 휴리스틱을 만드는 작업을 현재 진행형
5. 더 작고, 더 커스터마이징된 크롤러들은, 그들이 어떤 자원(서버, 네트워크 대역폭 등)에 얼마나 영향을 줄 것인지 스스로 제어할 수 있거나, 혹은 심지어 그자원들 자체가 크롤링을 수행하는 사람의 제어 하 있을 수 있으므로 피해갈 수 있음
6. 이 크롤러들은 문제를 예방하기 위해 사람의 모니터링에 더욱 의존
